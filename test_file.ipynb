{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Arrays have different shapes:', [(66, 7), (66,)])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Ensure all arrays have the same number of dimensions\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(array_shapes)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# If not, raise an error or handle the mismatch as appropriate\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArrays have different shapes:\u001b[39m\u001b[38;5;124m\"\u001b[39m, array_shapes)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Concatenate the landmarks data into a single numpy array\u001b[39;00m\n\u001b[0;32m     33\u001b[0m landmarks_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(landmarks_data, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: ('Arrays have different shapes:', [(66, 7), (66,)])"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import os\n",
    "\n",
    "# Directory containing the landmarks\n",
    "landmarks_dir = \"Face_Output/Face_Output_Split_Test\"\n",
    "\n",
    "# List all files in the landmarks directory\n",
    "landmarks_files = os.listdir(landmarks_dir)\n",
    "\n",
    "# Initialize an empty list to store landmarks data\n",
    "landmarks_data = []\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for file in landmarks_files:\n",
    "    # Check if the file is a numpy file\n",
    "    if file.endswith(\".npy\"):\n",
    "        # Load the landmarks data\n",
    "        landmarks_path = os.path.join(landmarks_dir, file)\n",
    "        landmarks = np.load(landmarks_path)\n",
    "        landmarks_data.append(landmarks)\n",
    "\n",
    "# Check the shapes of the loaded arrays\n",
    "array_shapes = [arr.shape for arr in landmarks_data]\n",
    "\n",
    "# Ensure all arrays have the same number of dimensions\n",
    "if len(set(array_shapes)) != 1:\n",
    "    # If not, raise an error or handle the mismatch as appropriate\n",
    "    raise ValueError(\"Arrays have different shapes:\", array_shapes)\n",
    "\n",
    "# Concatenate the landmarks data into a single numpy array\n",
    "landmarks_data = np.concatenate(landmarks_data, axis=0)\n",
    "\n",
    "# Now you have all the landmarks data in the variable landmarks_data\n",
    "\n",
    "# Check the shape of the landmarks_data array\n",
    "print(\"Shape of landmarks_data:\", landmarks_data.shape)\n",
    "\n",
    "# Prepare data: Flatten the input features (facial landmarks)\n",
    "# Assuming landmarks_data is a 2D array\n",
    "num_samples, landmark_length = landmarks_data.shape  # Get the shape directly\n",
    "X = landmarks_data.reshape(num_samples, -1)  # Flatten the input features\n",
    "\n",
    "# Random labels (replace with actual labels)\n",
    "y = np.random.randint(2, size=(num_samples,))\n",
    "\n",
    "# Define the neural network model\n",
    "model = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(\n",
    "        X.shape[1],)),  # Dense layer with ReLU activation\n",
    "    layers.Dropout(0.5),  # Dropout layer for regularization\n",
    "    # Output layer with sigmoid activation for binary classification\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "# Assuming you have a separate test dataset (X_test, y_test)\n",
    "# Replace X_test, y_test with your actual test dataset\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# Use the model for face recognition\n",
    "# Assuming you have new facial landmarks stored in a numpy array called new_landmarks\n",
    "# Replace new_landmarks with your actual new landmarks data\n",
    "predictions = model.predict(new_landmarks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
