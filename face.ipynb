{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import warnings\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Parsing Face Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directory where the face dataset is located\n",
    "base_directory = 'Dataset/VISA_Face/VISA_Face'\n",
    "\n",
    "# Initialize an empty list to store face images and their associated metadata\n",
    "face_images = []\n",
    "\n",
    "\n",
    "def parse_face_dataset():\n",
    "    # Clear the face_images list before processing the dataset\n",
    "    face_images.clear()\n",
    "\n",
    "    # Iterate over each directory in the base directory\n",
    "    for path in glob.iglob(base_directory + '/*'):\n",
    "        # Extract the filename from the path\n",
    "        filename = os.path.basename(path)\n",
    "\n",
    "        # Parse the filename to extract the label\n",
    "        underscore_index = filename.find(\"_\")\n",
    "        filename_parsed = filename[:underscore_index]\n",
    "        match = re.search(r\"(.*?)_2017_001\", filename)\n",
    "        if match:\n",
    "            filename_parsed = match.group(1)\n",
    "        else:\n",
    "            # Issue a warning if no match is found and skip processing this file\n",
    "            warnings.warn(f\"No match found for filename: {filename}\")\n",
    "            continue\n",
    "\n",
    "        # Assign the label parsed from the filename\n",
    "        label = filename_parsed\n",
    "        # Initialize an image ID counter\n",
    "        image_id = 0\n",
    "\n",
    "        # Iterate over each image file in the current directory\n",
    "        for image_path in glob.iglob(path + '/*'):\n",
    "            try:\n",
    "                # Read the image as grayscale\n",
    "                image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "                if image is None:\n",
    "                    # Issue a warning if the image fails to load and continue to the next image\n",
    "                    warnings.warn(f\"Failed to load image: {image_path}\")\n",
    "                    continue\n",
    "                # Resize the image to reduce memory usage\n",
    "                image = cv2.resize(image, (400, 300))\n",
    "                # Append the image, image ID, and label to the face_images list\n",
    "                face_images.append([image, image_id, label])\n",
    "                # Increment the image ID\n",
    "                image_id += 1\n",
    "            except Exception as e:\n",
    "                # Issue a warning if there's an error processing the image and continue to the next image\n",
    "                warnings.warn(f\"Error processing image: {image_path}\\n{e}\")\n",
    "\n",
    "    # Print the total number of face images found\n",
    "    print('Total Face Images Found: ' + str(len(face_images)))\n",
    "\n",
    "    # Return the list of face images and their associated metadata\n",
    "    return face_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Face Detection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detection(face_images, display):\n",
    "    # Initialize an empty list to store pre-processed images\n",
    "    pre_processed_images = []\n",
    "\n",
    "    # Load the pre-trained face cascade classifier\n",
    "    face_cascade = cv2.CascadeClassifier(\n",
    "        'Dependencies/haarcascade_frontalface_alt2.xml')\n",
    "\n",
    "    # Output directory for storing the detected faces\n",
    "    output_dir = os.path.join('Face_Output', 'Face_Output_Detection')\n",
    "\n",
    "    # Clear output directory if it already exists\n",
    "    if os.path.exists(output_dir):\n",
    "        for filename in os.listdir(output_dir):\n",
    "            file_path = os.path.join(output_dir, filename)\n",
    "            try:\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.unlink(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete {file_path}: {e}\")\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate over each face image in the input list\n",
    "    for face_image in face_images:\n",
    "        # Unpack the face image tuple into image, image_id, and label\n",
    "        (image, image_id, label) = face_image\n",
    "        image_id += 1  # Increment image ID\n",
    "\n",
    "        # Detect faces in the image using the cascade classifier\n",
    "        faces = face_cascade.detectMultiScale(image, 1.1, 4)\n",
    "\n",
    "        # Iterate over each detected face\n",
    "        for (x, y, width, height) in faces:\n",
    "            # Crop the detected face from the original image\n",
    "            face = image[y:y + height, x:x + width]\n",
    "\n",
    "            # Save the cropped face image to the output directory\n",
    "            output_path = os.path.join(\n",
    "                output_dir, f'{label}_{image_id}_Cropped.jpg')\n",
    "            cv2.imwrite(output_path, face)\n",
    "\n",
    "            # Append the cropped face image, image ID, and label to the pre_processed_images list\n",
    "            pre_processed_images.append([face, image_id, label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Face Feature Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def facial_feature_extraction(input_directory, output_dir):\n",
    "    # Initialize face detector and shape predictor\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor_path = 'Dependencies/shape_predictor_68_face_landmarks.dat'\n",
    "    predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = os.path.join(output_dir, 'Face_Output_Feature_Extraction')\n",
    "\n",
    "    # Clear output directory if it already exists\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)  # Remove the directory and its contents\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize lists to store features and labels\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate over images in the input directory\n",
    "    for filename in os.listdir(input_directory):\n",
    "        # Check if the file is an image (JPEG or PNG)\n",
    "        if filename.endswith('.jpg') or filename.endswith('.jpeg') or filename.endswith('.png'):\n",
    "            # Read the image\n",
    "            image_path = os.path.join(input_directory, filename)\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                continue\n",
    "\n",
    "            # Detect faces in the image\n",
    "            dets = detector(image, 1)\n",
    "\n",
    "            # Iterate over detected faces\n",
    "            for i, d in enumerate(dets):\n",
    "                # Predict facial landmarks\n",
    "                shape = predictor(image, d)\n",
    "\n",
    "                # Extract features\n",
    "                # Distance between the eyes\n",
    "                eye_distance = shape.part(45).x - shape.part(36).x\n",
    "                nose_shape = calculate_nose_shape(shape)  # Shape of the nose\n",
    "                lips_contour = calculate_lips_contour(\n",
    "                    shape)  # Contour of the lips\n",
    "                # Patterns of wrinkles around the mouth\n",
    "                mouth_wrinkles = calculate_mouth_wrinkles(shape)\n",
    "\n",
    "                # Append features to the feature vector\n",
    "                feature_vector = [eye_distance] + \\\n",
    "                    nose_shape + lips_contour + mouth_wrinkles\n",
    "\n",
    "                # Add feature vector and filename as label\n",
    "                features.append(feature_vector)\n",
    "                labels.append(filename)\n",
    "\n",
    "                # Draw lines between facial landmarks on the image\n",
    "                draw_lines(image, shape)\n",
    "\n",
    "            # Save image with landmarks and detected faces\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "            cv2.imwrite(output_path, image)\n",
    "\n",
    "    # Return extracted features and corresponding labels\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def calculate_eye_distance(shape):\n",
    "    # Calculate the Euclidean distance between the outer corners of the eyes\n",
    "    left_eye_outer_corner = (shape.part(36).x, shape.part(36).y)\n",
    "    right_eye_outer_corner = (shape.part(45).x, shape.part(45).y)\n",
    "    eye_distance = math.sqrt((right_eye_outer_corner[0] - left_eye_outer_corner[0])**2 + (\n",
    "        right_eye_outer_corner[1] - left_eye_outer_corner[1])**2)\n",
    "    return eye_distance\n",
    "\n",
    "\n",
    "def calculate_nose_shape(shape):\n",
    "    nose_shape = []\n",
    "    # Calculate the width of the nose\n",
    "    nose_width = shape.part(35).x - shape.part(31).x\n",
    "    # Calculate the height of the nose\n",
    "    nose_height = shape.part(50).y - shape.part(30).y\n",
    "    nose_shape.extend([nose_width, nose_height])\n",
    "    return nose_shape\n",
    "\n",
    "\n",
    "def calculate_lips_contour(shape):\n",
    "    lips_contour = []\n",
    "    # Calculate the width of the lips\n",
    "    lips_width = shape.part(54).x - shape.part(48).x\n",
    "    # Calculate the height of the lips\n",
    "    lips_height = shape.part(57).y - shape.part(51).y\n",
    "    lips_contour.extend([lips_width, lips_height])\n",
    "    return lips_contour\n",
    "\n",
    "\n",
    "def calculate_mouth_wrinkles(shape):\n",
    "    mouth_wrinkles = []\n",
    "    # Calculate the difference in y-coordinates between upper and lower lip\n",
    "    upper_lip_y = shape.part(51).y\n",
    "    lower_lip_y = shape.part(57).y\n",
    "    mouth_height = lower_lip_y - upper_lip_y\n",
    "    # Calculate the width of the mouth\n",
    "    mouth_width = shape.part(54).x - shape.part(48).x\n",
    "    mouth_wrinkles.extend([mouth_width, mouth_height])\n",
    "    return mouth_wrinkles\n",
    "\n",
    "\n",
    "def draw_lines(image, shape):\n",
    "    # Draw lines between specific facial landmarks\n",
    "    lines = [(30, 33), (48, 54), (48, 57), (36, 45)]  # Nose, lips, eyes\n",
    "    for start, end in lines:\n",
    "        # Draw a line between each pair of landmarks\n",
    "        cv2.line(image, (shape.part(start).x, shape.part(start).y),\n",
    "                 (shape.part(end).x, shape.part(end).y), (255, 0, 0), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Face Landmarks Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_facial_landmarks(input_dir, output_dir):\n",
    "    # Initialize face detector and shape predictor\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(\n",
    "        'Dependencies/shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = os.path.join(output_dir, 'Face_Output_Landmark_Extraction')\n",
    "\n",
    "    # Clear output directory if it already exists\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)  # Remove the directory and its contents\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate over images in the input directory\n",
    "    for filename in os.listdir(input_dir):\n",
    "        # Check if the file is an image (JPEG or PNG)\n",
    "        if filename.endswith('.jpg') or filename.endswith('.jpeg') or filename.endswith('.png'):\n",
    "            # Read the image\n",
    "            image_path = os.path.join(input_dir, filename)\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                continue\n",
    "\n",
    "            # Detect faces in the image\n",
    "            dets = detector(image, 1)\n",
    "\n",
    "            # Iterate over detected faces\n",
    "            for i, d in enumerate(dets):\n",
    "                # Predict facial landmarks\n",
    "                shape = predictor(image, d)\n",
    "                # Extract (x, y) coordinates of all 68 facial landmarks\n",
    "                landmarks = [(shape.part(i).x, shape.part(i).y)\n",
    "                             for i in range(68)]\n",
    "\n",
    "                # Draw landmarks on the image\n",
    "                for (x, y) in landmarks:\n",
    "                    cv2.circle(image, (x, y), 1, (0, 0, 255), -1)\n",
    "\n",
    "                # Save the image with landmarks\n",
    "                output_path = os.path.join(\n",
    "                    output_dir, f'{os.path.splitext(filename)[0]}_landmarks_{i}.jpg')\n",
    "                cv2.imwrite(output_path, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Landmarks to Feature Conversion Vectors Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def landmarks_to_features(landmarks, output_dir):\n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = os.path.join(output_dir, 'Face_Output_LFCV')\n",
    "\n",
    "    # Clear output directory if it already exists\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)  # Remove the directory and its contents\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate over each set of landmarks\n",
    "    for i, landmark_set in enumerate(landmarks):\n",
    "        # Flatten the landmark set into a feature vector\n",
    "        feature_vector = np.array(landmark_set).flatten()\n",
    "        # Define the output path for saving the feature vector\n",
    "        output_path = os.path.join(output_dir, f'landmarks_{i}.npy')\n",
    "        # Save the feature vector as a NumPy binary file\n",
    "        np.save(output_path, feature_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Splitting Data (80/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(features, labels, train_dir, test_dir):\n",
    "    # Clear existing directories if they exist\n",
    "    if os.path.exists(train_dir):\n",
    "        shutil.rmtree(train_dir)\n",
    "    if os.path.exists(test_dir):\n",
    "        shutil.rmtree(test_dir)\n",
    "\n",
    "    # Create directories\n",
    "    os.makedirs(train_dir)\n",
    "    os.makedirs(test_dir)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Save training data\n",
    "    np.save(os.path.join(train_dir, 'X_train.npy'), X_train)\n",
    "    np.save(os.path.join(train_dir, 'y_train.npy'), y_train)\n",
    "\n",
    "    # Save testing data\n",
    "    np.save(os.path.join(test_dir, 'X_test.npy'), X_test)\n",
    "    np.save(os.path.join(test_dir, 'y_test.npy'), y_test)\n",
    "\n",
    "    # Print the sizes of the training and testing sets\n",
    "    print(\"Training set size:\", len(X_train))\n",
    "    print(\"Testing set size:\", len(X_test))\n",
    "\n",
    "    # View saved data\n",
    "    print(\"\\nTraining Data:\")\n",
    "    view_saved_data(train_dir)\n",
    "\n",
    "    print(\"\\nTesting Data:\")\n",
    "    view_saved_data(test_dir)\n",
    "\n",
    "# Define the function to view saved data\n",
    "\n",
    "\n",
    "def view_saved_data(directory):\n",
    "    # Iterate over files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.npy'):  # Check if the file is a NumPy binary file\n",
    "            # Construct the file path\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            # Load the data from the file\n",
    "            data = np.load(filepath)\n",
    "            # Print filename and shape of the loaded data\n",
    "            print(f\"Filename: {filename}, Shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def face_data(): USE ONLY WHEN COMBINING THE DATASETS\n",
    "if __name__ == \"__main__\":\n",
    "    # PHASE 1 - Parse the face dataset\n",
    "    face_images = parse_face_dataset()\n",
    "    print(\"Parsing Face Dataset STARTED...\")\n",
    "    print(\"Parsing Face Dataset COMPLETE!\")\n",
    "\n",
    "    # PHASE 2 - Perform face detection\n",
    "    print(\"Face Image Preprocessing STARTED...\")\n",
    "    # Suppress display for face detection\n",
    "    face_detection(face_images, display=False)\n",
    "    print(\"Face Image Preprocessing COMPLETE!\")\n",
    "\n",
    "    # PHASE 3 - Perform facial feature extraction\n",
    "    print(\"Face Feature Extraction STARTED...\")\n",
    "    # Input directory containing images with detected faces\n",
    "    input_directory_detection = 'Face_Output/Face_Output_Detection'\n",
    "    # Output directory for saving images with landmarks and detected faces\n",
    "    output_directory_feature_extraction = 'Face_Output'\n",
    "    features, labels = facial_feature_extraction(\n",
    "        input_directory_detection, output_directory_feature_extraction)\n",
    "    print(\"Face Feature Extraction COMPLETE!\")\n",
    "\n",
    "    # PHASE 4 - Extract facial landmarks from an image\n",
    "    print(\"Extracting facial landmarks STARTED...\")\n",
    "    # Input directory containing images with extracted facial features\n",
    "    input_directory_extraction = 'Face_Output/Face_Output_Feature_Extraction'\n",
    "    # Output directory for saving images with facial landmarks\n",
    "    output_directory_landmarks = 'Face_Output'\n",
    "    extract_facial_landmarks(\n",
    "        input_directory_extraction, output_directory_landmarks)\n",
    "    print(\"Extracting facial landmarks COMPLETE!\")\n",
    "\n",
    "    # PHASE 5 - Convert facial landmarks into feature vectors\n",
    "    print(\"Converting facial landmarks to feature vectors STARTED...\")\n",
    "    landmarks_to_features(features, output_dir='Face_Output')\n",
    "    print(\"Converting facial landmarks to feature vectors COMPLETE!\")\n",
    "    print(\"Number of feature vectors generated:\", len(\n",
    "        os.listdir('Face_Output/Face_Output_LFCV')))\n",
    "\n",
    "    # PHASE 6 - Splitting Data (80% Training, 20% Test)\n",
    "    # Define directory paths\n",
    "    train_dir = 'Face_Output/Face_Output_Split_Train'\n",
    "    test_dir = 'Face_Output/Face_Output_Split_Test'\n",
    "\n",
    "    # Split the data\n",
    "    split_data(features, labels, train_dir, test_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
