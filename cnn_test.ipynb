{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 380\n",
      "Number of test images: 95\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - 21s 2s/step - loss: 6.0339 - accuracy: 0.0000e+00 - val_loss: 5.9405 - val_accuracy: 0.0105\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - 25s 2s/step - loss: 5.9420 - accuracy: 0.0026 - val_loss: 5.9401 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - 24s 2s/step - loss: 5.9415 - accuracy: 0.0026 - val_loss: 5.9425 - val_accuracy: 0.0105\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - 24s 2s/step - loss: 5.9422 - accuracy: 0.0026 - val_loss: 5.9656 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - 24s 2s/step - loss: 5.9347 - accuracy: 0.0026 - val_loss: 6.0086 - val_accuracy: 0.0105\n",
      "Epoch 6/20\n",
      "12/12 [==============================] - 24s 2s/step - loss: 5.9242 - accuracy: 0.0053 - val_loss: 5.9549 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/20\n",
      "12/12 [==============================] - 24s 2s/step - loss: 5.8471 - accuracy: 0.0026 - val_loss: 6.0267 - val_accuracy: 0.0105\n",
      "Epoch 8/20\n",
      "12/12 [==============================] - 23s 2s/step - loss: 5.6413 - accuracy: 0.0184 - val_loss: 6.1440 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/20\n",
      "12/12 [==============================] - 23s 2s/step - loss: 5.1609 - accuracy: 0.0526 - val_loss: 6.6750 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/20\n",
      "12/12 [==============================] - 23s 2s/step - loss: 4.4747 - accuracy: 0.0974 - val_loss: 8.1106 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/20\n",
      "12/12 [==============================] - 23s 2s/step - loss: 3.5812 - accuracy: 0.2132 - val_loss: 7.9207 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/20\n",
      "12/12 [==============================] - 24s 2s/step - loss: 2.6815 - accuracy: 0.3868 - val_loss: 10.3331 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/20\n",
      "12/12 [==============================] - 23s 2s/step - loss: 1.7356 - accuracy: 0.5763 - val_loss: 15.8498 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/20\n",
      "12/12 [==============================] - 23s 2s/step - loss: 1.0425 - accuracy: 0.7079 - val_loss: 13.1954 - val_accuracy: 0.0105\n",
      "Epoch 15/20\n",
      "12/12 [==============================] - 24s 2s/step - loss: 0.8776 - accuracy: 0.7895 - val_loss: 15.0664 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/20\n",
      "12/12 [==============================] - 24s 2s/step - loss: 0.7099 - accuracy: 0.8211 - val_loss: 17.4547 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/20\n",
      "12/12 [==============================] - 23s 2s/step - loss: 0.3863 - accuracy: 0.9000 - val_loss: 19.3251 - val_accuracy: 0.0105\n",
      "Epoch 18/20\n",
      "12/12 [==============================] - 22s 2s/step - loss: 0.3004 - accuracy: 0.9289 - val_loss: 17.6682 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/20\n",
      "12/12 [==============================] - 20s 2s/step - loss: 0.2289 - accuracy: 0.9395 - val_loss: 17.3984 - val_accuracy: 0.0105\n",
      "Epoch 20/20\n",
      "12/12 [==============================] - 19s 2s/step - loss: 0.2759 - accuracy: 0.9474 - val_loss: 18.9044 - val_accuracy: 0.0105\n",
      "3/3 [==============================] - 1s 413ms/step - loss: 18.9044 - accuracy: 0.0105\n",
      "Test Accuracy: 0.010526316240429878\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load image data\n",
    "\n",
    "\n",
    "def load_images(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.jpg'):\n",
    "            img = cv2.imread(os.path.join(\n",
    "                directory, filename), cv2.IMREAD_GRAYSCALE)\n",
    "            if img is not None:\n",
    "                img = cv2.resize(img, (300, 400))  # Resize image to (300, 400)\n",
    "                images.append(img)\n",
    "                # Extract label from the entire file name\n",
    "                # Extracts the filename without extension\n",
    "                label = os.path.splitext(filename)[0]\n",
    "                labels.append(label)\n",
    "    # Return list of original file names as labels\n",
    "    return np.array(images), labels\n",
    "\n",
    "\n",
    "# Define the base directory where the face dataset is located\n",
    "base_directory = \"Face_Output\"\n",
    "train_dir = \"Face_Cropped_Split/Train\"\n",
    "test_dir = \"Face_Cropped_Split/Test\"  # Adjusted test directory\n",
    "\n",
    "print(\"Number of training images:\", len(\n",
    "    os.listdir(os.path.join(base_directory, train_dir))))\n",
    "print(\"Number of test images:\", len(\n",
    "    os.listdir(os.path.join(base_directory, test_dir))))\n",
    "\n",
    "# Load training data\n",
    "X_train, y_train = load_images(os.path.join(base_directory, train_dir))\n",
    "\n",
    "# Check the number of unique classes in the training data\n",
    "# num_classes = len(np.unique(y_train))\n",
    "# print(\"Number of unique classes:\", num_classes)\n",
    "\n",
    "# Check the unique labels in the training data\n",
    "# unique_labels = np.unique(y_train)\n",
    "# print(\"Unique labels:\", unique_labels)\n",
    "\n",
    "# Normalize pixel values to range [0, 1]\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "\n",
    "# Reshape input data to match model input shape\n",
    "X_train = np.expand_dims(X_train, axis=-1)  # Add a single channel dimension\n",
    "\n",
    "# Define the CNN architecture for facial recognition\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(400, 300, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    # Output layer with softmax activation\n",
    "    # Ensure num_classes matches the number of unique labels\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Load test data\n",
    "X_test, y_test = load_images(os.path.join(base_directory, test_dir))\n",
    "\n",
    "# Normalize pixel values to range [0, 1]\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Reshape test data to match model input shape\n",
    "X_test = np.expand_dims(X_test, axis=-1)  # Add a single channel dimension\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, np.arange(len(y_train)), epochs=15, validation_data=(\n",
    "    X_test, np.arange(len(y_test))))  # Pass numerical labels\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(\n",
    "    X_test, np.arange(len(y_test)))  # Pass numerical labels\n",
    "\n",
    "# Convert accuracy to percentage\n",
    "test_acc_percentage = test_acc * 100\n",
    "\n",
    "print(\"Test Accuracy: {:.2f}%\".format(test_acc_percentage))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
